{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train = open('archive (2)/train.txt', 'r')\n",
    "test = open('archive (2)/test.txt', 'r')\n",
    "val = open('archive (2)/val.txt', 'r')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "train_ = []\n",
    "test_ = []\n",
    "val_ = []\n",
    "for i in train:\n",
    "    data = i.split(';')\n",
    "    train_.append([data[0], data[1]])\n",
    "for i in test:\n",
    "    data = i.split(';')\n",
    "    test_.append([data[0], data[1]])\n",
    "for i in val:\n",
    "    data = i.split(';')\n",
    "    val_.append([data[0], data[1]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame(train_)\n",
    "test_ = pd.DataFrame(test_)\n",
    "val_ = pd.DataFrame(val_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_, test_, val_], axis=0)\n",
    "labels = full_df[1].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      0  1\n0                          [i, didnt, feel, humiliated]  0\n1     [i, can, go, from, feeling, so, hopeless, to, ...  0\n2     [im, grabbing, a, minute, to, post, i, feel, g...  1\n3     [i, am, ever, feeling, nostalgic, about, the, ...  2\n4                             [i, am, feeling, grouchy]  1\n...                                                 ... ..\n1995  [im, having, ssa, examination, tomorrow, in, t...  0\n1996  [i, constantly, worry, about, their, fight, ag...  5\n1997  [i, feel, its, important, to, share, this, inf...  5\n1998  [i, truly, feel, that, if, you, are, passionat...  5\n1999  [i, feel, like, i, just, wan, na, buy, any, cu...  5\n\n[20000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[i, didnt, feel, humiliated]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[i, can, go, from, feeling, so, hopeless, to, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[im, grabbing, a, minute, to, post, i, feel, g...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[i, am, ever, feeling, nostalgic, about, the, ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[i, am, feeling, grouchy]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>[im, having, ssa, examination, tomorrow, in, t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>[i, constantly, worry, about, their, fight, ag...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>[i, feel, its, important, to, share, this, inf...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>[i, truly, feel, that, if, you, are, passionat...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>[i, feel, like, i, just, wan, na, buy, any, cu...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>20000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "labels_int = {labels[i]: i for i in range(len(labels))}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "train_[1] = train_[1].map(labels_int)\n",
    "test_[1] = test_[1].map(labels_int)\n",
    "val_[1] = val_[1].map(labels_int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "train_[0] = train_[0].map(word_tokenize)\n",
    "test_[0] = test_[0].map(word_tokenize)\n",
    "val_[0] = val_[0].map(word_tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "punct = set(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stops]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "full_df[0] = full_df[0].map(word_tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "full_df[1] = full_df[1].map(labels_int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      0  1\n0                          [i, didnt, feel, humiliated]  0\n1     [i, can, go, from, feeling, so, hopeless, to, ...  0\n2     [im, grabbing, a, minute, to, post, i, feel, g...  1\n3     [i, am, ever, feeling, nostalgic, about, the, ...  2\n4                             [i, am, feeling, grouchy]  1\n...                                                 ... ..\n1995  [im, having, ssa, examination, tomorrow, in, t...  0\n1996  [i, constantly, worry, about, their, fight, ag...  5\n1997  [i, feel, its, important, to, share, this, inf...  5\n1998  [i, truly, feel, that, if, you, are, passionat...  5\n1999  [i, feel, like, i, just, wan, na, buy, any, cu...  5\n\n[20000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[i, didnt, feel, humiliated]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[i, can, go, from, feeling, so, hopeless, to, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[im, grabbing, a, minute, to, post, i, feel, g...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[i, am, ever, feeling, nostalgic, about, the, ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[i, am, feeling, grouchy]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>[im, having, ssa, examination, tomorrow, in, t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>[i, constantly, worry, about, their, fight, ag...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>[i, feel, its, important, to, share, this, inf...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>[i, truly, feel, that, if, you, are, passionat...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>[i, feel, like, i, just, wan, na, buy, any, cu...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>20000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words_counter = Counter()\n",
    "for i in full_df[0]:\n",
    "    for j in i:\n",
    "        words_counter[j] += 1\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "words_most_common = words_counter.most_common()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "words_most_common_indices = {words_most_common[i - 1][0]: i for i in range(1, len(words_most_common) + 1)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "train2int = []\n",
    "for i, label in zip(train_[0], train_[1]):\n",
    "    _ = []\n",
    "    for j in i:\n",
    "        _.append(words_most_common_indices[j])\n",
    "    train2int.append([_, label])\n",
    "\n",
    "test2int = []\n",
    "for i, label in zip(test_[0], test_[1]):\n",
    "    _ = []\n",
    "    for j in i:\n",
    "        _.append(words_most_common_indices[j])\n",
    "    test2int.append([_, label])\n",
    "\n",
    "val2int = []\n",
    "for i, label in zip(val_[0], val_[1]):\n",
    "    _ = []\n",
    "    for j in i:\n",
    "        _.append(words_most_common_indices[j])\n",
    "    val2int.append([_, label])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from keras.src.layers import Embedding, SimpleRNN, Dense\n",
    "#\n",
    "# model_new = Sequential()\n",
    "#\n",
    "# model_new.add(Embedding(17094, 8))\n",
    "# model_new.add(SimpleRNN(8))\n",
    "# model_new.add(Dense(1, activation='sigmoid'))\n",
    "#\n",
    "# model_new.compile(\n",
    "#     # loss=keras.losses.BinaryCrossentropy(),\n",
    "#     loss='binary_crossentropy',\n",
    "#     optimizer='adam',\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "X_train_embs = [i[0] for i in train2int]\n",
    "y_train = [i[1] for i in train2int]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "X_test_embs = [i[0] for i in test2int]\n",
    "y_test = [i[1] for i in test2int]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_x_train = pad_sequences(\n",
    "    X_train_embs,\n",
    "    maxlen=200,\n",
    ")\n",
    "\n",
    "padded_x_test = pad_sequences(\n",
    "    X_test_embs,\n",
    "    maxlen=200,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# X_test_embs_first1000 = []\n",
    "# X_train_embs_first1000 = []\n",
    "# for i in X_test_embs:\n",
    "#     _ = [j for j in i if j <= 1000]\n",
    "#     X_test_embs_first1000.append(_)\n",
    "#\n",
    "# for i in X_train_embs:\n",
    "#     _ = [j for j in i if j <= 1000]\n",
    "#     X_train_embs_first1000.append(_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16000,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[105], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m padded_x_train \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_embs_first1000\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m padded_x_test \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(X_test_embs_first1000)\n",
      "\u001B[0;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16000,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# padded_x_train = np.array(X_train_embs_first1000)\n",
    "# padded_x_test = np.array(X_test_embs_first1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 200, 8)            136752    \n",
      "                                                                 \n",
      " simple_rnn_8 (SimpleRNN)    (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136897 (534.75 KB)\n",
      "Trainable params: 136897 (534.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 12s 20ms/step - loss: -5.5034 - accuracy: 0.1349 - val_loss: -10.0111 - val_accuracy: 0.1375\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 10s 19ms/step - loss: -13.6072 - accuracy: 0.1349 - val_loss: -17.4086 - val_accuracy: 0.1375\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 9s 18ms/step - loss: -20.7710 - accuracy: 0.1349 - val_loss: -24.5406 - val_accuracy: 0.1375\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 9s 19ms/step - loss: -27.7701 - accuracy: 0.1349 - val_loss: -31.5650 - val_accuracy: 0.1375\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 9s 18ms/step - loss: -34.6968 - accuracy: 0.1349 - val_loss: -38.5418 - val_accuracy: 0.1375\n"
     ]
    }
   ],
   "source": [
    "from keras.src.layers import Embedding, SimpleRNN\n",
    "\n",
    "model_new = Sequential()\n",
    "\n",
    "model_new.add(Embedding(17094, 8, input_length=200))\n",
    "model_new.add(SimpleRNN(8))\n",
    "model_new.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model_new.compile(\n",
    "    # loss=keras.losses.BinaryCrossentropy(),\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_new.summary()\n",
    "history_ = model_new.fit(padded_x_train, np.array(y_train),\n",
    "                         epochs=5, verbose=1, validation_data=(padded_x_test, np.array(y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "model_ = Sequential()\n",
    "model_.add(LSTM(64, activation='relu', input_shape=(200, 1)))\n",
    "model_.add(Dense(1, activation='softmax'))\n",
    "model_.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "# padded_x_train = tf.convert_to_tensor(padded_x_train)\n",
    "# padded_x_test = tf.convert_to_tensor(padded_x_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "# padded_x_train_ = np.expand_dims(padded_x_train, 2)\n",
    "# padded_x_test_ = np.expand_dims(padded_x_test, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 19s 37ms/step - loss: -163.2444 - accuracy: 0.1349 - val_loss: -505.3679 - val_accuracy: 0.1375\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 17s 34ms/step - loss: nan - accuracy: 0.1849 - val_loss: nan - val_accuracy: 0.2905\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 20s 40ms/step - loss: nan - accuracy: 0.2916 - val_loss: nan - val_accuracy: 0.2905\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 22s 43ms/step - loss: nan - accuracy: 0.2916 - val_loss: nan - val_accuracy: 0.2905\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 19s 38ms/step - loss: nan - accuracy: 0.2916 - val_loss: nan - val_accuracy: 0.2905\n"
     ]
    }
   ],
   "source": [
    "_history = model_.fit(padded_x_train, np.array(y_train),\n",
    "                      epochs=5, verbose=1, validation_data=(padded_x_test, np.array(y_test)), )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[   0,    0,    0, ..., 4974,  111,   58],\n       [   0,    0,    0, ...,    1,    2,  457],\n       [   0,    0,    0, ...,  395,   24,   67],\n       ...,\n       [   0,    0,    0, ...,    7,    9, 3512],\n       [   0,    0,    0, ...,   49,   10, 2678],\n       [   0,    0,    0, ...,    4,  294, 1964]], dtype=int32)"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}